{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus2question example\n",
    "====================\n",
    "\n",
    "This notebook modifies the `corpus2question` tutorial to generate questions from sets of Wikipedia pages. \n",
    "\n",
    "* See the original code repository here: https://github.com/unicamp-dl/corpus2question\n",
    "* And the corresponding paper: https://arxiv.org/abs/2009.09290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Functions from the corpus2question tutorial \n",
    "import c2q\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Here we create a function to fetch the underlying text for a list of Wikipedia pages. Note the Wikipedia API produces extracts not the the entire page but this should be enough for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_corpus(wiki_pages):\n",
    "    for page in wiki_pages:\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&titles={page}&prop=extracts&exintro&explaintext\"\n",
    "        rsp = requests.get(url)\n",
    "        data = rsp.json()\n",
    "        for _id, details in data[\"query\"][\"pages\"].items():\n",
    "            yield details[\"extract\"]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"Bob_Dylan\",\n",
    "    \"Woody_Guthrie\",\n",
    "    \"Pete_Seeger\",\n",
    "    \"Bessie_Smith\",\n",
    "    \"Levon_Helm\",\n",
    "    \"Bruce_Springsteen\"\n",
    "]\n",
    "# pages = [\n",
    "#     \"Computer\",\n",
    "#     \"Internet\",\n",
    "#     \"Software\",\n",
    "#     \"Operating_System\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass our list of pages to our our Wikipedia function and then pass that iterable of text to the original `corpus2questions` code to generate a list of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_wiki_corpus(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "questions = c2q.get_questions(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(q) for q in [d for d in questions]]), \"total questions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with Pandas\n",
    "\n",
    "This follows the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df = pd.DataFrame([\n",
    "    dict(\n",
    "        document_id=doc_idx,\n",
    "        span_id=f\"{doc_idx}:{span_idx}\",\n",
    "        gen_id=f\"{doc_idx}:{span_idx}:{gen_idx}\",\n",
    "        question=question,\n",
    "    )\n",
    "    for doc_idx, document_gen in enumerate(questions)\n",
    "    for span_idx, span_gen in enumerate(document_gen)\n",
    "    for gen_idx, question in enumerate(span_gen)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the results by question, count unique results and order by generation id counts.\n",
    "question_df \\\n",
    "    .groupby(\"question\") \\\n",
    "    .nunique() \\\n",
    "    .sort_values(\"gen_id\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": null
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
